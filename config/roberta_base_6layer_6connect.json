{
    "attention_probs_dropout_prob": 0.1,
    "finetuning_task": null,
    "hidden_act": "gelu",
    "hidden_dropout_prob": 0.1,
    "hidden_size": 768,
    "initializer_range": 0.02,
    "intermediate_size": 3072,
    "layer_norm_eps": 1e-12,
    "max_position_embeddings": 514,
    "num_attention_heads": 12,
    "num_hidden_layers": 12,
    "num_labels": 2,
    "output_attentions": false,
    "output_hidden_states": false,
    "torchscript": false,
    "type_vocab_size": 1,
    "vocab_size": 50265,
    "model": "roberta",
    "v_feature_size": 2048,
    "v_target_size": 1601,
    "v_hidden_size": 1024,
    "v_num_hidden_layers": 6,
    "v_num_attention_heads": 8,
    "v_intermediate_size": 1024,
    "bi_hidden_size": 1024,
    "bi_num_attention_heads": 8,
    "bi_intermediate_size": 1024,
    "bi_attention_type": 1,
    "v_attention_probs_dropout_prob": 0.1,
    "v_hidden_act": "gelu",
    "v_hidden_dropout_prob": 0.1,
    "v_initializer_range": 0.02,
    "v_biattention_id": [
        0,
        1,
        2,
        3,
        4,
        5
    ],
    "t_biattention_id": [
        6,
        7,
        8,
        9,
        10,
        11
    ],
    "pooling_method": "mul"
}